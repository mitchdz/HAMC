#!/bin/bash

### script to run an mpi job using 28 cores or less (using only one 28-core node)

### Set the job name
#PBS -N HAMC

### Specify the group for this job
### List of PI groups available to each user can be found with "va" command
#PBS -W group_list=ece569

### Set the queue for this job as windfall or standard (adjust ### and #)
#PBS -q standard

### Set the number of nodes, cores and memory that will be used for this job
### select=1 is the node count, ncpus=28 are the cores in each node,
### mem=168gb is memory per node, pcmem=6gb is the memory per core - optional

###PBS -l select=1:ncpus=28:mem=168gb:ngpus1
#PBS -l select=1:ncpus=2:mem=12gb:ngpus=1
### Specify "wallclock time", hhh:mm:ss. Required field
#PBS -l walltime=00:00:10

### Specify total cpu time, hhh:mm:ss. Calculated for you if left out
### total cputime = walltime * ncpus
#PBS -l cput=00:00:10

module load openmpi
module load cuda91/toolkit/9.1.85

BUILD_DIR="/home/u1/mitchdz/git/HAMC/build"

cd ${BUILD_DIR}

echo "Running unit tests" > unit_tests.txt

echo "Binary Matrix Multiplication" >> unit_tests.txt
./Multiply_test -t -x 1024 -y 1024 >> unit_tests.txt

#echo "Insert Kernel" >> unit_tests.txt
#./kernel_test_call -options >> unit_tests.txt
