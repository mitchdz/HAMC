#!/bin/bash

### script to run an mpi job using 28 cores or less (using only one 28-core node)

### Set the job name
#PBS -N HAMC

### Specify the group for this job
### List of PI groups available to each user can be found with "va" command
#PBS -W group_list=ece569

### Set the queue for this job as windfall or standard (adjust ### and #)
#PBS -q standard

### Set the number of nodes, cores and memory that will be used for this job
### select=1 is the node count, ncpus=28 are the cores in each node,
### mem=168gb is memory per node, pcmem=6gb is the memory per core - optional

###PBS -l select=1:ncpus=28:mem=168gb:ngpus1
#PBS -l select=1:ncpus=2:mem=12gb:ngpus=1
### Specify "wallclock time", hhh:mm:ss. Required field
#PBS -l walltime=00:00:10

### Specify total cpu time, hhh:mm:ss. Calculated for you if left out
### total cputime = walltime * ncpus
#PBS -l cput=00:00:10

module load openmpi
module load cuda91/toolkit/9.1.85

BUILD_DIR="/home/u1/mitchdz/git/HAMC/build"

cd ${BUILD_DIR}

./hamc -a test > hamc_output_gpu.txt
./hamc -a test -c > hamc_output_cpu.txt

